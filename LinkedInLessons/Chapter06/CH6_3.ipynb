{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C70PhPKgtKj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def load_bert_model():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    return tokenizer, model\n",
        "\n",
        "def get_word_embedding(word, tokenizer, model):\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "\n",
        "    # Check if we're using BertForMaskedLM or BertModel\n",
        "    if isinstance(model, BertForMaskedLM):\n",
        "        # Get the base BERT model from the MLM model\n",
        "        outputs = model.bert(**inputs)\n",
        "    else:\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    return outputs.last_hidden_state[0][1:-1].mean(dim=0).detach().numpy()\n",
        "\n",
        "def analyze_gender_bias(model=None, tokenizer=None):\n",
        "    if model is None or tokenizer is None:\n",
        "        tokenizer, model = load_bert_model()\n",
        "\n",
        "    # Define gender-specific word pairs\n",
        "    male_words = [\"he\", \"man\", \"father\", \"son\", \"brother\", \"uncle\"]\n",
        "    female_words = [\"she\", \"woman\", \"mother\", \"daughter\", \"sister\", \"aunt\"]\n",
        "\n",
        "    # Define profession words\n",
        "    professions = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"scientist\", \"assistant\"]\n",
        "\n",
        "    # Get embeddings\n",
        "    male_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in male_words])\n",
        "    female_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in female_words])\n",
        "    profession_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in professions])\n",
        "\n",
        "    # Calculate gender direction\n",
        "    gender_direction = (male_embeddings.mean(axis=0) - female_embeddings.mean(axis=0))\n",
        "    gender_direction = gender_direction / np.linalg.norm(gender_direction)\n",
        "\n",
        "    # Calculate bias scores for professions\n",
        "    bias_scores = {}\n",
        "    for prof, emb in zip(professions, profession_embeddings):\n",
        "        # Project profession embedding onto gender direction\n",
        "        bias = np.dot(emb, gender_direction)\n",
        "        bias_scores[prof] = bias\n",
        "\n",
        "    return bias_scores\n",
        "\n",
        "class GenderNeutralDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.training_texts = [\n",
        "            \"The doctor performed surgery.\",\n",
        "            \"The nurse helped the patient.\",\n",
        "            \"The engineer designed the bridge.\",\n",
        "            \"The teacher educated the students.\",\n",
        "            \"The scientist conducted research.\",\n",
        "            \"The assistant organized the meeting.\"\n",
        "        ]\n",
        "\n",
        "        self.encodings = tokenizer(\n",
        "            self.training_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training_texts)\n",
        "\n",
        "def fine_tune_bert():\n",
        "    \"\"\"Fine-tune BERT on gender-neutral dataset\"\"\"\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GenderNeutralDataset(tokenizer)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./bert_fine_tuned\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        logging_steps=10,\n",
        "        save_steps=50,\n",
        "        learning_rate=2e-5\n",
        "    )\n",
        "\n",
        "    # Create trainer and fine-tune\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer\n",
        "\n",
        "# Demonstrate bias before and after fine-tuning\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Original BERT bias scores:\")\n",
        "    original_bias = analyze_gender_bias()\n",
        "    for prof, score in original_bias.items():\n",
        "        print(f\"{prof}: {score:.3f}\")\n",
        "\n",
        "    # Fine-tune BERT\n",
        "    fine_tuned_model, tokenizer = fine_tune_bert()\n",
        "\n",
        "    print(\"\\nFine-tuned BERT bias scores:\")\n",
        "    fine_tuned_bias = analyze_gender_bias(fine_tuned_model, tokenizer)\n",
        "    for prof, score in fine_tuned_bias.items():\n",
        "        print(f\"{prof}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def load_bert_model():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    return tokenizer, model\n",
        "\n",
        "def get_word_embedding(word, tokenizer, model):\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "\n",
        "    # Check if we're using BertForMaskedLM or BertModel\n",
        "    if isinstance(model, BertForMaskedLM):\n",
        "        # Get the base BERT model from the MLM model\n",
        "        outputs = model.bert(**inputs)\n",
        "    else:\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    return outputs.last_hidden_state[0][1:-1].mean(dim=0).detach().numpy()\n",
        "\n",
        "def analyze_gender_bias(model=None, tokenizer=None):\n",
        "    if model is None or tokenizer is None:\n",
        "        tokenizer, model = load_bert_model()\n",
        "\n",
        "    # Define gender-specific word pairs\n",
        "    male_words = [\"he\", \"man\", \"father\", \"son\", \"brother\", \"uncle\"]\n",
        "    female_words = [\"she\", \"woman\", \"mother\", \"daughter\", \"sister\", \"aunt\"]\n",
        "\n",
        "    # Define profession words - expanded list\n",
        "    professions = [\n",
        "        \"doctor\", \"nurse\", \"engineer\", \"teacher\", \"scientist\", \"assistant\",\n",
        "        \"lawyer\", \"secretary\", \"programmer\", \"chef\", \"pilot\", \"writer\",\n",
        "        \"ceo\", \"receptionist\", \"professor\", \"carpenter\", \"accountant\", \"manager\"\n",
        "    ]\n",
        "\n",
        "    # Get embeddings\n",
        "    male_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in male_words])\n",
        "    female_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in female_words])\n",
        "    profession_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in professions])\n",
        "\n",
        "    # Calculate gender direction with verification\n",
        "    male_center = male_embeddings.mean(axis=0)\n",
        "    female_center = female_embeddings.mean(axis=0)\n",
        "\n",
        "    # Verify the norms of the centers\n",
        "    print(f\"Male center norm: {np.linalg.norm(male_center):.3f}\")\n",
        "    print(f\"Female center norm: {np.linalg.norm(female_center):.3f}\")\n",
        "\n",
        "    # Calculate and normalize gender direction\n",
        "    gender_direction = (male_center - female_center)\n",
        "    gender_direction = gender_direction / np.linalg.norm(gender_direction)\n",
        "\n",
        "    # Verify gender direction is working as expected\n",
        "    male_proj = np.dot(male_center, gender_direction)\n",
        "    female_proj = np.dot(female_center, gender_direction)\n",
        "    print(f\"\\nGender direction verification:\")\n",
        "    print(f\"Male center projection: {male_proj:.3f}\")\n",
        "    print(f\"Female center projection: {female_proj:.3f}\")\n",
        "\n",
        "    # Calculate and analyze raw embeddings\n",
        "    profession_embeddings = np.array([get_word_embedding(w, tokenizer, model) for w in professions])\n",
        "\n",
        "    # Compute pairwise similarities between professions\n",
        "    similarities = cosine_similarity(profession_embeddings)\n",
        "\n",
        "    print(\"\\nRaw embedding analysis:\")\n",
        "    print(\"\\n1. Embedding Magnitudes (higher = stronger semantic representation):\")\n",
        "    mags = [(prof, np.linalg.norm(emb)) for prof, emb in zip(professions, profession_embeddings)]\n",
        "    mags.sort(key=lambda x: x[1], reverse=True)\n",
        "    for prof, mag in mags:\n",
        "        print(f\"{prof:15} {mag:.3f}\")\n",
        "\n",
        "    print(\"\\n2. Most Similar Profession Pairs (cosine similarity > 0.6):\")\n",
        "    pairs = []\n",
        "    for i in range(len(professions)):\n",
        "        for j in range(i+1, len(professions)):\n",
        "            sim = similarities[i][j]\n",
        "            if sim > 0.6:\n",
        "                pairs.append((professions[i], professions[j], sim))\n",
        "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    for p1, p2, sim in pairs:\n",
        "        print(f\"{p1:15} - {p2:15} {sim:.3f}\")\n",
        "\n",
        "    # Calculate bias scores\n",
        "    bias_scores = {}\n",
        "    for prof, emb in zip(professions, profession_embeddings):\n",
        "        # Project profession embedding onto gender direction\n",
        "        bias = np.dot(emb, gender_direction)\n",
        "        bias_scores[prof] = bias\n",
        "\n",
        "    return bias_scores\n",
        "\n",
        "class GenderNeutralDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.training_texts = [\n",
        "            \"The doctor performed surgery.\",\n",
        "            \"The nurse helped the patient.\",\n",
        "            \"The engineer designed the bridge.\",\n",
        "            \"The teacher educated the students.\",\n",
        "            \"The scientist conducted research.\",\n",
        "            \"The assistant organized the meeting.\"\n",
        "        ]\n",
        "\n",
        "        self.encodings = tokenizer(\n",
        "            self.training_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training_texts)\n",
        "\n",
        "def fine_tune_bert():\n",
        "    \"\"\"Fine-tune BERT on gender-neutral dataset\"\"\"\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GenderNeutralDataset(tokenizer)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./bert_fine_tuned\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        logging_steps=10,\n",
        "        save_steps=50,\n",
        "        learning_rate=2e-5\n",
        "    )\n",
        "\n",
        "    # Create trainer and fine-tune\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer\n",
        "\n",
        "# Demonstrate bias before and after fine-tuning\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Original BERT bias scores:\")\n",
        "    original_bias = analyze_gender_bias()\n",
        "    for prof, score in original_bias.items():\n",
        "        print(f\"{prof}: {score:.3f}\")\n",
        "\n",
        "    # Fine-tune BERT\n",
        "    fine_tuned_model, tokenizer = fine_tune_bert()\n",
        "\n",
        "    print(\"\\nFine-tuned BERT bias scores:\")\n",
        "    fine_tuned_bias = analyze_gender_bias(fine_tuned_model, tokenizer)\n",
        "    for prof, score in fine_tuned_bias.items():\n",
        "        print(f\"{prof}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "KW2hjbpmi7b1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}